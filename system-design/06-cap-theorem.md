## âœ… CAP Theorem â€” Interview-Ready Explanation

### ğŸ”¹ What is the CAP Theorem?

The **CAP Theorem**, proposed by **Eric Brewer**, states that in any **distributed system**, you can **only achieve two
out of the following three guarantees simultaneously**:

| Property                    | Description                                                                                                |
|-----------------------------|------------------------------------------------------------------------------------------------------------|
| **Consistency (C)**         | Every read receives the most recent write (or an error). All nodes see the same data at the same time.     |
| **Availability (A)**        | Every request receives a (non-error) response â€” even if it's not the most recent. System stays responsive. |
| **Partition Tolerance (P)** | The system continues to operate despite network partitions (message delays or losses between nodes).       |

> In the presence of a **network partition**, a system must choose between **Consistency** and **Availability**.

---

### ğŸ”¹ Why is Partition Tolerance Non-Negotiable?

- Distributed systems **must** tolerate partitions â€” network failures and latency are inevitable.
- So, practically, **CAP becomes a choice between Consistency and Availability during a partition**.

---

### ğŸ”¹ CAP Trade-offs â€” Choose Any Two

| Type   | Guarantees                         | Trade-Off                                                                 | Examples                                                   |
|--------|------------------------------------|---------------------------------------------------------------------------|------------------------------------------------------------|
| **CP** | Consistency + Partition Tolerance  | Sacrifices availability if partitioned                                    | HBase, MongoDB (strong consistency), Redis (with Sentinel) |
| **AP** | Availability + Partition Tolerance | Sacrifices consistency (eventual)                                         | Cassandra, Couchbase, DynamoDB                             |
| **CA** | Consistency + Availability         | Sacrifices partition tolerance (not possible in real distributed systems) | Single-node RDBMS (no partitioning)                        |

---

### ğŸ”¹ Real-World Analogies

- **CP**: Bank transaction system â†’ Always correct, even if some servers are down.
- **AP**: Twitter feed â†’ Better to show stale data than be unavailable.
- **CA**: Not realistic in a distributed system â€” assumes perfect networks (no partitions).

---

### ğŸ”¹ Interview Questions You Might Face

**Q1. Why canâ€™t a distributed system achieve all three?**
> Because during a partition, to stay available, a system must risk serving stale data. To stay consistent, it may need
> to reject requests. You canâ€™t do both.

**Q2. Which trade-off is better â€” CP or AP?**
> It depends on the use case:
> - **CP** if correctness matters (e.g., financial systems).
> - **AP** if user experience matters (e.g., social media, shopping cart).

**Q3. Can consistency be relaxed?**
> Yes â€” many systems adopt **eventual consistency** to stay available and partition-tolerant, eventually syncing data
> across nodes.

**Q4. How does Cassandra balance CAP?**
> Cassandra is **AP** by default but offers tunable consistency via quorum reads/writes, allowing trade-offs per
> operation.

---

### ğŸ”¹ Bonus: PACELC Model (Advanced Interview Tip)

While CAP focuses only on what happens **during a partition**, the **PACELC model** extends it:

> **PACELC** = _If there is a Partition (P), choose between Availability (A) and Consistency (C); **Else**, choose
between Latency (L) and Consistency (C)._

This gives a more nuanced view, even when the system is functioning normally.

---

### ğŸ§  Summary Cheat Sheet

| System    | Partition Tolerance | Availability | Consistency | Notes                                |
|-----------|---------------------|--------------|-------------|--------------------------------------|
| Cassandra | âœ… Yes               | âœ… Yes        | âŒ Eventual  | Tunable consistency via quorum       |
| MongoDB   | âœ… Yes               | âŒ Sometimes  | âœ… Strong    | Primary-secondary with write concern |
| DynamoDB  | âœ… Yes               | âœ… Yes        | âŒ Eventual  | Can be tuned to strong consistency   |
| HBase     | âœ… Yes               | âŒ No         | âœ… Strong    | Designed for consistency             |